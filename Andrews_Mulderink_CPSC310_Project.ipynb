{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alexa Andrews and Jeffrey Mulderink  \n",
    "#### Group name: aa_jm_knn   \n",
    "#### Project title: Improved kNN classifier\n",
    "\n",
    "\n",
    "Our data set was looking to predict whether a person would default on their credit card payment based\n",
    "Since our table has around 30 thousand instances and kNN is rather slow, we used a random subset of our data in our tests.  \n",
    "We thought we should remove the first column of our table, which was ID, because the order these instances happen to be in shouldn't be of relevance to whether they will make their next payment, and it was not included on the list of attributes for the dataset. Initially we had ID and oddly enough removing it led to lower recall, precision, and F-measure for different k values. Without removing ID, it was often amongst the top attributes in the top subset. From visual inspection of our data, it did not appear that the class label was sorted in any way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "header, table = utils.open_csv_with_header(\"default_of_credit_card_clients.csv\")\n",
    "\n",
    "np.random.shuffle(table)\n",
    "table = table[:250]\n",
    "\n",
    "header = header[1:]\n",
    "table = utils.remove_column(table, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_attribute_subset(table, header, num_values):\n",
    "    '''\n",
    "        Returns a copy table with a random columns removed\n",
    "        Param table: A table to remove attributes from\n",
    "        Param header: The attribute names\n",
    "        Param num_values: The number of attributes to keep\n",
    "        Returns: A tuple with the first item being the table with num_values random attibutes\n",
    "                and the second item a list of the names of the attributes it chose\n",
    "    '''\n",
    "    smaller_table = copy.deepcopy(table)\n",
    "    num_attributes = len(smaller_table[0])\n",
    "    indices_to_remove = random.sample(range(0, num_attributes-1), num_attributes-num_values) \n",
    "    indices_to_remove.sort(reverse=True)\n",
    "    for c in indices_to_remove:\n",
    "        for r, _ in enumerate(smaller_table):\n",
    "            del smaller_table[r][c] \n",
    "        \n",
    "    attributes_kept = [header[i] for i in range(num_attributes) if i not in indices_to_remove]\n",
    "        \n",
    "    return smaller_table, attributes_kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more class 0 than class 1. This zero_R_classifier tests the accuracy (other metrics don't make sense with TP=0) of just predicting 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806\n"
     ]
    }
   ],
   "source": [
    "def zero_r(table):\n",
    "    '''\n",
    "        A zero rules classifier which returns the most common class\n",
    "        Param table:  A table with classifications of instances in the last row\n",
    "        Returns: The most frequent class\n",
    "    '''\n",
    "    classes, counts = utils.get_frequencies(table, -1)\n",
    "    combo = [(counts[i], classes[i]) for i in range(len(classes))]\n",
    "    combo.sort(reverse=True)\n",
    "    return combo[0][1]\n",
    "\n",
    "\n",
    "\n",
    "def zero_R_classifier(table, measurement='a'):\n",
    "    folds = utils.get_stratified_folds(table)\n",
    "    prediction = zero_r(table)\n",
    "\n",
    "    predictions, actuals = [], [] \n",
    "    for i, fold in enumerate(folds):\n",
    "        train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "        test, train = utils.normalize_attributes(fold, train)\n",
    "        for test_instance in test:\n",
    "            predictions.append(prediction)\n",
    "            actuals.append(test_instance[-1])\n",
    "\n",
    "\n",
    "    correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "    return correct.count(True) / len(correct)\n",
    "\n",
    "\n",
    "print(zero_R_classifier(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following cell tests how different k values impact the performace of a kNN classifier. We were surprised by how large K was for optimal results. Before removing ID, these forms of classifier evaluation all tended to be highest in the upper 60 to 100 range, after which they would drop off. After removing ID, around 100 was best for accuracy and recall, but precision and f-measure were much higher, so much so as to be basically using the majority from all training instances. At this point, we created the zero-R classifier above which showed that these Ks were not achieving the same accuracy as simply guessing the majority class, 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing at k=91\n",
      "testing at k=97\n",
      "testing at k=103\n",
      "testing at k=109\n",
      "testing at k=115\n",
      "testing at k=121\n",
      "testing at k=127\n",
      "testing at k=133\n",
      "testing at k=139\n",
      "testing at k=145\n",
      "testing at k=151\n",
      "testing at k=157\n",
      "testing at k=163\n",
      "testing at k=169\n",
      "testing at k=175\n",
      "testing at k=181\n",
      "testing at k=187\n",
      "testing at k=193\n",
      "testing at k=199\n",
      "Accuracies for variable k\n",
      " [(0.78, 91), (0.78, 97), (0.78, 103), (0.78, 109), (0.78, 115), (0.78, 121), (0.78, 127), (0.78, 133), (0.78, 139), (0.78, 145), (0.78, 151), (0.78, 157), (0.78, 163), (0.78, 169), (0.78, 175), (0.78, 181), (0.78, 187), (0.78, 193), (0.78, 199)]\n",
      "sorted [(0.78, 199), (0.78, 193), (0.78, 187), (0.78, 181), (0.78, 175), (0.78, 169), (0.78, 163), (0.78, 157), (0.78, 151), (0.78, 145), (0.78, 139), (0.78, 133), (0.78, 127), (0.78, 121), (0.78, 115), (0.78, 109), (0.78, 103), (0.78, 97), (0.78, 91)]\n"
     ]
    }
   ],
   "source": [
    "def create_kNN_classifier_vary_k(table, start_k=9, end_k=99, step=6, measurement='a'):\n",
    "    '''\n",
    "        This function uses stratified cross fold validation to test different k values for a table.\n",
    "        It can return measurements of accuracy ('a'), recall('r'), precision('p'), or F-measure('f')\n",
    "        Param table: A table to test kNN on\n",
    "        Param start_k: The minimum k value to test.\n",
    "        Param end_k: The maximum k value to test\n",
    "        Param step: The step between k values tested. \n",
    "        Param measurement: The measurement type to return \n",
    "                            accuracy ('a'), recall('r'), precision('p'), or F-measure('f')\n",
    "        Returns: A list of tuples (measurement_value, k)\n",
    "    '''\n",
    "    folds = utils.get_stratified_folds(table)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for k in range(start_k, end_k, step):\n",
    "        print(\"testing at k=%d\" % k)\n",
    "        predictions, actuals = [], [] \n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            utils.remove_column(train, -1) # remove the class column before prediction\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance[:-1], train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "        \n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            results.append((correct.count(True) / len(correct), k))\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                results.append((true_positives.count(True)/predicted_positives,k))\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                results.append((true_positives.count(True)/actual_positives,k))\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                results.append((2*precision*recall/(precision+recall),k))\n",
    "            else:\n",
    "                print(\"error - invalid measurement\", measurement)\n",
    "                break\n",
    "    return results\n",
    "\n",
    "\n",
    "accuracies_k = create_kNN_classifier_vary_k(table, start_k=91, end_k=200)\n",
    "print(\"Accuracies for variable k\\n\", accuracies_k)\n",
    "accuracies_k.sort(reverse=True)\n",
    "print(\"sorted\", accuracies_k)\n",
    "\n",
    "# recalls_k = create_kNN_classifier_vary_k(table, 91, 110, measurement='r')\n",
    "# # print(\"Recall values for variable k\\n\", recalls_k)\n",
    "# recalls_k.sort(reverse=True)\n",
    "# print(\"sorted\", recalls_k)\n",
    "\n",
    "# precisions_k = create_kNN_classifier_vary_k(table, 91, 110, measurement='p')\n",
    "# # print(\"Precision values for variable k\\n\", precisions_k)\n",
    "# precisions_k.sort(reverse=True)\n",
    "# print(\"sorted\", precisions_k)\n",
    "\n",
    "# f_measures_k = create_kNN_classifier_vary_k(table, 91, 110, measurement='f')\n",
    "# # print(\"F-measure values for variable k\\n\", f_measures_k)\n",
    "# f_measures_k.sort(reverse=True)\n",
    "# print(\"sorted\", f_measures_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classifier uses the best K determined by the previous step and varies which attributes it uses for prediction by taking random attribute subsets of size, default 10, which is passed as a parameter. It does this multiple times and evaluates them using either accuracy, recall, precision, or F-measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing random attribute set 1 of 30\n",
      "testing random attribute set 2 of 30\n",
      "testing random attribute set 3 of 30\n",
      "testing random attribute set 4 of 30\n",
      "testing random attribute set 5 of 30\n",
      "testing random attribute set 6 of 30\n",
      "testing random attribute set 7 of 30\n",
      "testing random attribute set 8 of 30\n",
      "testing random attribute set 9 of 30\n",
      "testing random attribute set 10 of 30\n",
      "testing random attribute set 11 of 30\n",
      "testing random attribute set 12 of 30\n",
      "testing random attribute set 13 of 30\n",
      "testing random attribute set 14 of 30\n",
      "testing random attribute set 15 of 30\n",
      "testing random attribute set 16 of 30\n",
      "testing random attribute set 17 of 30\n",
      "testing random attribute set 18 of 30\n",
      "testing random attribute set 19 of 30\n",
      "testing random attribute set 20 of 30\n",
      "testing random attribute set 21 of 30\n",
      "testing random attribute set 22 of 30\n",
      "testing random attribute set 23 of 30\n",
      "testing random attribute set 24 of 30\n",
      "testing random attribute set 25 of 30\n",
      "testing random attribute set 26 of 30\n",
      "testing random attribute set 27 of 30\n",
      "testing random attribute set 28 of 30\n",
      "testing random attribute set 29 of 30\n",
      "testing random attribute set 30 of 30\n",
      "\n",
      "Best 5 accuracies for variable attribute subset\n",
      " [(0.804, ['LIMIT_BAL', 'SEX', 'PAY_0', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT3', 'BILL_AMT5', 'PAY_AMT1', 'default payment next month']), (0.802, ['LIMIT_BAL', 'PAY_0', 'PAY_6', 'BILL_AMT1', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT6', 'default payment next month']), (0.8, ['SEX', 'AGE', 'PAY_0', 'PAY_4', 'PAY_6', 'BILL_AMT2', 'PAY_AMT2', 'PAY_AMT4', 'PAY_AMT5', 'default payment next month']), (0.8, ['LIMIT_BAL', 'PAY_0', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT2', 'BILL_AMT4', 'PAY_AMT1', 'PAY_AMT2', 'default payment next month']), (0.8, ['LIMIT_BAL', 'MARRIAGE', 'AGE', 'PAY_2', 'BILL_AMT3', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT3', 'default payment next month'])]\n",
      "[0, 1, 5, 9, 10, 11, 13, 15, 17, 23]\n"
     ]
    }
   ],
   "source": [
    "def create_kNN_classifier_vary_attributes(table, header, k, iterations=20, F=10, measurement='a'):\n",
    "    '''\n",
    "        Param table: The table used to test different attributes from\n",
    "        Param k: number of nearest neighbors\n",
    "        Param iterations: number of random subsets of attributes tested\n",
    "        Param F: number of attributes per subset\n",
    "        Param measurement: The measurement type to return \n",
    "                            accuracy ('a'), recall('r'), precision('p'), or F-measure('f')\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    for i in range(iterations):\n",
    "        print(\"testing random attribute set\", i+1, \"of\", iterations)\n",
    "        current_table, current_attribs = get_random_attribute_subset(table, header, F)\n",
    "        folds = utils.get_stratified_folds(current_table)\n",
    "        predictions, actuals = [], []\n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            utils.remove_column(train, -1) # remove the class column before prediction\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance[:-1], train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            results.append((correct.count(True) / len(correct), current_attribs))\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                results.append((true_positives.count(True)/predicted_positives, current_attribs))\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                results.append((true_positives.count(True)/actual_positives, current_attribs))\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                results.append((2*precision*recall/(precision+recall), current_attribs))\n",
    "    \n",
    "    return results\n",
    "\n",
    "accuracies_a = create_kNN_classifier_vary_attributes(table, header, accuracies_k[0][1], 30)\n",
    "accuracies_a.sort(reverse=True)\n",
    "print(\"\\nBest 5 accuracies for variable attribute subset\\n\", accuracies_a[:5])\n",
    "best_feature_set_indices = [header.index(x) for x in accuracies_a[0][1]]\n",
    "print(best_feature_set_indices)\n",
    "\n",
    "\n",
    "# recalls_a = create_kNN_classifier_vary_attributes(table, header, recalls_k[0][1], 30, measurement='r')\n",
    "# recalls_a.sort(reverse=True)\n",
    "# print(\"\\nBest 5 recall values for variable attribute subset\\n\", recalls_a[:5])\n",
    "# best_feature_set_indices = [header.index(x) for x in recalls_a[0][1]]\n",
    "# print(best_feature_set_indices)\n",
    "\n",
    "# precisions_a = create_kNN_classifier_vary_attributes(table, header, precisions_k[0][1], 30, measurement='r')\n",
    "# precisions_a.sort(reverse=True)\n",
    "# print(\"\\nBest 5 precision values for variable attribute subset\\n\", precisions_a[:5])\n",
    "# best_feature_set_indices = [header.index(x) for x in precisions_a[0][1]]\n",
    "# print(best_feature_set_indices)\n",
    "\n",
    "# f_measures_a = create_kNN_classifier_vary_attributes(table, header, f_measures_k[0][1], 30, measurement='r')\n",
    "# f_measures_a.sort(reverse=True)\n",
    "# print(\"\\nBest 5 precision values for variable attribute subset\\n\", f_measures_a[:5])\n",
    "# best_feature_set_indices = [header.index(x) for x in f_measures_a[0][1]]\n",
    "# print(best_feature_set_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_of_table(table, attributes):\n",
    "    smaller_table = copy.deepcopy(table)\n",
    "    num_attributes = len(smaller_table[0])\n",
    "    indices_to_remove = [i for i in range(num_attributes) if i not in attributes]\n",
    "    indices_to_remove.sort(reverse=True)\n",
    "    for c in indices_to_remove:\n",
    "        for r, _ in enumerate(smaller_table):\n",
    "            del smaller_table[r][c] \n",
    "        \n",
    "    return smaller_table\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_table(table, weights):\n",
    "    '''\n",
    "        Weights the columns of a table by the amount given by weights\n",
    "        Param table: A table of data, recommended normalized\n",
    "        Param weights: The weights to apply by column to the table\n",
    "        Returns: A table with the columns of the original table multipled by the given weights\n",
    "    '''\n",
    "    for r in range(len(table)):\n",
    "        for c in range(len(weights)):\n",
    "            table[r][c] = table[r][c] * weights[c]\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this following cell, using the previously calculated best k and attribute subset, we try different weights for each subset. This method allowed us to achieve accuracy in the high 90s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing random attribute weights 1 of 30\n",
      "testing random attribute weights 2 of 30\n",
      "testing random attribute weights 3 of 30\n",
      "testing random attribute weights 4 of 30\n",
      "testing random attribute weights 5 of 30\n",
      "testing random attribute weights 6 of 30\n",
      "testing random attribute weights 7 of 30\n",
      "testing random attribute weights 8 of 30\n",
      "testing random attribute weights 9 of 30\n",
      "testing random attribute weights 10 of 30\n",
      "testing random attribute weights 11 of 30\n",
      "testing random attribute weights 12 of 30\n",
      "testing random attribute weights 13 of 30\n",
      "testing random attribute weights 14 of 30\n",
      "testing random attribute weights 15 of 30\n",
      "testing random attribute weights 16 of 30\n",
      "testing random attribute weights 17 of 30\n",
      "testing random attribute weights 18 of 30\n",
      "testing random attribute weights 19 of 30\n",
      "testing random attribute weights 20 of 30\n",
      "testing random attribute weights 21 of 30\n",
      "testing random attribute weights 22 of 30\n",
      "testing random attribute weights 23 of 30\n",
      "testing random attribute weights 24 of 30\n",
      "testing random attribute weights 25 of 30\n",
      "testing random attribute weights 26 of 30\n",
      "testing random attribute weights 27 of 30\n",
      "testing random attribute weights 28 of 30\n",
      "testing random attribute weights 29 of 30\n",
      "testing random attribute weights 30 of 30\n",
      "\n",
      "Best 5 accuracies for variable weights over attribute subset\n",
      " [(0.99, {'LIMIT_BAL': 0, 'SEX': 1, 'PAY_0': 1, 'PAY_5': 4, 'PAY_6': 0, 'BILL_AMT1': 5, 'BILL_AMT3': 1, 'BILL_AMT5': 2, 'PAY_AMT1': 1, 'default payment next month': 1}), (0.99, {'LIMIT_BAL': 3, 'SEX': 4, 'PAY_0': 0, 'PAY_5': 1, 'PAY_6': 3, 'BILL_AMT1': 0, 'BILL_AMT3': 0, 'BILL_AMT5': 4, 'PAY_AMT1': 3, 'default payment next month': 1}), (0.988, {'LIMIT_BAL': 2, 'SEX': 1, 'PAY_0': 4, 'PAY_5': 0, 'PAY_6': 4, 'BILL_AMT1': 0, 'BILL_AMT3': 2, 'BILL_AMT5': 4, 'PAY_AMT1': 1, 'default payment next month': 1}), (0.988, {'LIMIT_BAL': 0, 'SEX': 0, 'PAY_0': 1, 'PAY_5': 3, 'PAY_6': 5, 'BILL_AMT1': 1, 'BILL_AMT3': 2, 'BILL_AMT5': 4, 'PAY_AMT1': 3, 'default payment next month': 1}), (0.988, {'LIMIT_BAL': 1, 'SEX': 2, 'PAY_0': 1, 'PAY_5': 0, 'PAY_6': 5, 'BILL_AMT1': 0, 'BILL_AMT3': 3, 'BILL_AMT5': 4, 'PAY_AMT1': 1, 'default payment next month': 1})]\n"
     ]
    }
   ],
   "source": [
    "def create_kNN_classifier_vary_weights(table, header, attributes, k, iterations=20, measurement='a'):\n",
    "    '''\n",
    "        Param table: The table used for testing classification\n",
    "        Param header: The names of all the attributes\n",
    "        Param attributes: The attribute subset to use\n",
    "        Param k: the number of nearest neighbors to use\n",
    "        Param iterations: The number of random weights to test\n",
    "        Param measurement: The type of measurement to calculate\n",
    "            accuracy ('a'), recall('r'), precision('p'), or F-measure('f')\n",
    "        Returns: A tuple with the calculated measurement value as the first item and a dictionary associating attribute\n",
    "                names with weights for the secon\n",
    "    '''\n",
    "    best_feature_set_indices = [header.index(x) for x in attributes]\n",
    "    table = get_subset_of_table(table, best_feature_set_indices)\n",
    "    results = []\n",
    "    for i in range(iterations):\n",
    "        print(\"testing random attribute weights\", i+1, \"of\", iterations)\n",
    "        folds = utils.get_stratified_folds(table)\n",
    "        predictions, actuals = [], []\n",
    "        weights = [random.randint(0,5) for _ in range(len(best_feature_set_indices)-1)] # weight all but class label (-1)\n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            train = weight_table(train, weights)\n",
    "            test = weight_table(test, weights)\n",
    "            utils.remove_column(train, -1) # remove the class column before prediction\n",
    "            utils.remove_column(test, -1)\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance, train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "        weights.append(1)  # weight of class label is just one, so same length as attributes\n",
    "        weights = dict(zip(attributes, weights))  # associate attributes with their weights for results\n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            results.append((correct.count(True) / len(correct), weights))\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                weights = dict(zip(attributes))\n",
    "                results.append((true_positives.count(True)/predicted_positives, weights))\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                results.append((true_positives.count(True)/actual_positives, weights))\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                results.append((2*precision*recall/(precision+recall), weights))\n",
    "        \n",
    "    return results\n",
    "        \n",
    "\n",
    "\n",
    "accuracies_w = create_kNN_classifier_vary_weights(table, header, accuracies_a[0][1], accuracies_k[0][1], 30)\n",
    "accuracies_w.sort(key=lambda x:x[0], reverse=True)\n",
    "print(\"\\nBest 5 accuracies for variable weights over attribute subset\\n\", accuracies_w[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble classifier uses different k-values combined with different random attribute subsets and different random weights to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing weak learner 1 of 30\n",
      "Testing weak learner 2 of 30\n",
      "Testing weak learner 3 of 30\n",
      "Testing weak learner 4 of 30\n",
      "Testing weak learner 5 of 30\n",
      "Testing weak learner 6 of 30\n",
      "Testing weak learner 7 of 30\n",
      "Testing weak learner 8 of 30\n",
      "Testing weak learner 9 of 30\n",
      "Testing weak learner 10 of 30\n",
      "Testing weak learner 11 of 30\n",
      "Testing weak learner 12 of 30\n",
      "Testing weak learner 13 of 30\n",
      "Testing weak learner 14 of 30\n",
      "Testing weak learner 15 of 30\n",
      "Testing weak learner 16 of 30\n",
      "Testing weak learner 17 of 30\n",
      "Testing weak learner 18 of 30\n",
      "Testing weak learner 19 of 30\n",
      "Testing weak learner 20 of 30\n",
      "Testing weak learner 21 of 30\n",
      "Testing weak learner 22 of 30\n",
      "Testing weak learner 23 of 30\n",
      "Testing weak learner 24 of 30\n",
      "Testing weak learner 25 of 30\n",
      "Testing weak learner 26 of 30\n",
      "Testing weak learner 27 of 30\n",
      "Testing weak learner 28 of 30\n",
      "Testing weak learner 29 of 30\n",
      "Testing weak learner 30 of 30\n",
      "[[0.992, 65, [2, 8, 20, 22, 23], [3, 4, 1, 3]], [0.988, 119, [7, 9, 17, 22, 23], [2, 2, 2, 3]], [0.988, 51, [0, 4, 11, 12, 17, 19, 23], [2, 2, 4, 2, 0, 3]], [0.984, 73, [5, 6, 14, 18, 23], [4, 3, 1, 5]], [0.984, 45, [0, 4, 10, 20, 23], [4, 4, 4, 1]], [0.984, 67, [6, 8, 14, 15, 16, 23], [1, 4, 0, 2, 0]], [0.976, 99, [1, 4, 6, 10, 11, 14, 17, 23], [1, 2, 3, 5, 1, 2, 1]], [0.976, 47, [1, 5, 7, 9, 12, 13, 14, 18, 23], [3, 1, 0, 2, 2, 1, 2, 4]], [0.976, 67, [0, 2, 3, 4, 6, 8, 10, 11, 13, 14, 16, 18, 23], [5, 3, 0, 1, 4, 0, 5, 0, 2, 4, 3, 3]], [0.976, 51, [0, 4, 5, 8, 17, 19, 20, 21, 23], [1, 1, 3, 1, 3, 0, 0, 2]], [0.976, 97, [2, 3, 7, 8, 9, 12, 14, 21, 23], [0, 2, 4, 0, 2, 0, 0, 2]], [0.976, 113, [1, 4, 9, 11, 16, 21, 22, 23], [4, 2, 3, 1, 1, 5, 3]], [0.972, 79, [4, 6, 8, 10, 11, 12, 13, 15, 17, 19, 21, 23], [1, 5, 0, 3, 1, 4, 3, 2, 3, 1, 1]], [0.972, 113, [3, 7, 9, 16, 17, 23], [3, 2, 0, 5, 1]], [0.972, 53, [3, 8, 10, 17, 21, 23], [5, 2, 4, 1, 0]]]\n",
      "0.744\n"
     ]
    }
   ],
   "source": [
    "def create_ensemble_classifier(table, header, N=30, M=15, measurement='a'):\n",
    "    '''\n",
    "        Creates N weak classifiers and uses majority voting of the best M to make predictions. \n",
    "    '''\n",
    "    weak_classifiers = []  # list of [measurement, k, attribute indices, weights]\n",
    "\n",
    "    for i in range(N):\n",
    "        print(\"Testing weak learner\", i+1, \"of\", N)\n",
    "        k = random.randint(20,70)\n",
    "        k = 2*k + 1 # ensure k is odd\n",
    "        current_table, current_attribs = get_random_attribute_subset(table, header, random.randint(5, 15)) # random attribute subset\n",
    "        best_feature_set_indices = [header.index(x) for x in current_attribs]\n",
    "        weights = [random.randint(0,5) for _ in range(len(best_feature_set_indices)-1)]\n",
    "\n",
    "        folds = utils.get_stratified_folds(current_table)\n",
    "        predictions, actuals = [], []\n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            train = weight_table(train, weights)\n",
    "            test = weight_table(test, weights)\n",
    "            utils.remove_column(train, -1) # remove the class column before prediction\n",
    "            utils.remove_column(test, -1)\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance, train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "\n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            weak_classifiers.append([correct.count(True) / len(correct), k, best_feature_set_indices, weights])\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                weights = dict(zip(attributes))\n",
    "                weak_classifiers.append([true_positives.count(True)/predicted_positives, k, best_feature_set_indices, weights])\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                weak_classifiers.append([true_positives.count(True)/actual_positives, k, best_feature_set_indices, weights])\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                weak_classifiers.append([2*precision*recall/(precision+recall), k, best_feature_set_indices, weights])\n",
    "        weak_classifiers.sort(key=lambda x:x[0], reverse=True)\n",
    "\n",
    "    \n",
    "    ensemble = weak_classifiers[:M]\n",
    "    return ensemble\n",
    "    \n",
    "    \n",
    "def test_ensemble_classifier(table, ensemble, measurement='a'):\n",
    "    '''\n",
    "        Param table: A table to test the ensemble classifier over\n",
    "        Param ensemble: A list of weak kNN learners represented by a list [measurement, k, attribute indices, weights]\n",
    "    '''\n",
    "    folds = utils.get_stratified_folds(table)\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    for i, fold in enumerate(folds):\n",
    "            weak_classifiers_predictions = [[] for _ in fold]  # a list of predictions for each test instance\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            for classifier in ensemble:\n",
    "                training_set = get_subset_of_table(train, classifier[2])\n",
    "                test_set = get_subset_of_table(fold, classifier[2])\n",
    "                test_set, training_set = utils.normalize_attributes(test_set, training_set)\n",
    "                training_set = weight_table(training_set, classifier[3])\n",
    "                test_set = weight_table(test_set, classifier[3])\n",
    "                utils.remove_column(training_set, -1) # remove the class column before prediction\n",
    "                utils.remove_column(test_set, -1)\n",
    "                for i, test_instance in enumerate(test_set):\n",
    "                    weak_classifiers_predictions[i].append(utils.make_kNN_prediction(test_instance, training_set, classifier[1]))\n",
    "            predictions.extend([int(np.median(x)) for x in weak_classifiers_predictions])  # binary classification majority voting\n",
    "            actuals.extend(utils.get_column(fold, -1))  # actual class labels\n",
    "    \n",
    "    if measurement == 'a':\n",
    "        correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "        return correct.count(True) / len(correct)\n",
    "    else:\n",
    "        true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "        if measurement == 'r':\n",
    "            predicted_positives = predictions.count(1)\n",
    "            return true_positives.count(True)/predicted_positives\n",
    "        elif measurement == 'p':\n",
    "            actual_positives = actuals.count(1)\n",
    "            return true_positives.count(True)/actual_positives\n",
    "        elif measurement == 'f':\n",
    "            recall = true_positives.count(True) / predictions.count(1)\n",
    "            precision = true_positives.count(True) / actuals.count(1)\n",
    "            return 2*precision*recall/(precision+recall)\n",
    "    \n",
    "random_ensemble = create_ensemble_classifier(table, header)\n",
    "print(random_ensemble)\n",
    "\n",
    "print(test_ensemble_classifier(table, random_ensemble))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
