{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alexa Andrews and Jeffrey Mulderink  \n",
    "#### Group name: aa_jm_knn   \n",
    "#### Project title: Improved kNN classifier\n",
    "\n",
    "\n",
    "Our data set was looking to predict whether a person would default on their credit card payment based on gender, age, marital status and education level, as well as the original amount of credit. For each month Sept-April, it there are attributes for their history of payment (whether they paid on time or how far delayed their payment is), the amount of their last payment, and the amount of their bill. The class label we are trying to predict is whether they will default on their payment the next month.\n",
    "\n",
    "Since our table has around 30 thousand instances and kNN is rather slow, we used a random subset of our data in our tests.  \n",
    "We thought we should remove the first column of our table, which was ID, because the order these instances happen to be in shouldn't be of relevance to whether they will make their next payment, and it was not included on the list of attributes for the dataset. Initially we had ID and oddly enough removing it led to lower recall, precision, and F-measure for different k values. Without removing ID, it was often amongst the top attributes in the top subset. From visual inspection of our data, it did not appear that the class label was sorted in any way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "header, table = utils.open_csv_with_header(\"default_of_credit_card_clients.csv\")\n",
    "\n",
    "np.random.shuffle(table)\n",
    "table = table[:250]\n",
    "\n",
    "header = header[1:]\n",
    "table = utils.remove_column(table, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-05f8906677fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pie_chart_high_defaulting.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m \u001b[0mpie_charts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-05f8906677fe>\u001b[0m in \u001b[0;36mpie_charts\u001b[1;34m(table)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mgender_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#female\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[0mgender_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "header, table1 = utils.open_csv_with_header(\"default_of_credit_card_clients.csv\")\n",
    "\n",
    "def pie_charts(table1):\n",
    "    gender_freq = [0,0,0,0]\n",
    "    marriage_freq = [0,0,0,0]\n",
    "    education_freq = [0,0,0,0,0,0]\n",
    "    labels = [\"Default\",\"No Default\"]\n",
    "    for row in table1:\n",
    "        if row[2]==1:#male\n",
    "            if row[24]==1:\n",
    "                gender_freq[1]+=1\n",
    "            else:\n",
    "                gender_freq[0]+=1\n",
    "        if row[2]==2:#female\n",
    "            if row[24]==1:\n",
    "                gender_freq[3]+=1\n",
    "            else:\n",
    "                gender_freq[2]+=1\n",
    "        if row[2]==1:#married\n",
    "            if row[24]==1:\n",
    "                marriage_freq[1]+=1\n",
    "            else:\n",
    "                marriage_freq[0]+=1\n",
    "        if row[2]==2:#single\n",
    "            if row[24]==1:\n",
    "                marriage_freq[3]+=1\n",
    "            else:\n",
    "                marriage_freq[2]+=1\n",
    "        if row[3]==1:#grad\n",
    "            if row[24]==1:\n",
    "                education_freq[1]+=1\n",
    "            else:\n",
    "                education_freq[0]+=1\n",
    "        if row[3]==2:#uni\n",
    "            if row[24]==1:\n",
    "                education_freq[3]+=1\n",
    "            else:\n",
    "                education_freq[2]+=1\n",
    "        if row[3]==3:#high school\n",
    "            if row[24]==1:\n",
    "                education_freq[5]+=1\n",
    "            else:\n",
    "                education_freq[4]+=1\n",
    "    #male defaultvs nondefault\n",
    "    plt.figure()\n",
    "    plt.pie([gender_freq[1],gender_freq[0]],labels=labels,autopct=\"%1.2f%%\")\n",
    "    plt.title(\"Male\")\n",
    "    plt.savefig(\"pie_chart_male_defaulting.png\")        \n",
    "\n",
    "    plt.figure()\n",
    "    plt.pie([gender_freq[3],gender_freq[2]],labels=labels,autopct=\"%1.2f%%\")\n",
    "    plt.title(\"Female\")\n",
    "    plt.savefig(\"pie_chart_female_defaulting.png\")   \n",
    "\n",
    "    plt.figure()\n",
    "    plt.pie([marriage_freq[1],marriage_freq[0]],labels=labels,autopct=\"%1.2f%%\")\n",
    "    plt.title(\"Married\")\n",
    "    plt.savefig(\"pie_chart_married_defaulting.png\")        \n",
    "\n",
    "    plt.figure()\n",
    "    plt.pie([marriage_freq[3],marriage_freq[2]],labels=labels,autopct=\"%1.2f%%\")\n",
    "    plt.title(\"Single\")\n",
    "    plt.savefig(\"pie_chart_single_defaulting.png\")  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.pie([education_freq[1],education_freq[0]],labels=labels,autopct=\"%1.2f%%\")\n",
    "    plt.title(\"Graduate School\")\n",
    "    plt.savefig(\"pie_chart_grad_defaulting.png\") \n",
    "\n",
    "    plt.figure()\n",
    "    plt.pie([education_freq[3],education_freq[2]],labels=labels,autopct=\"%1.2f%%\")\n",
    "    plt.title(\"University\")\n",
    "    plt.savefig(\"pie_chart_uni_defaulting.png\")        \n",
    "\n",
    "    plt.figure()\n",
    "    plt.pie([education_freq[5],education_freq[4]],labels=labels,autopct=\"%1.2f%%\")\n",
    "    plt.title(\"High School\")\n",
    "    plt.savefig(\"pie_chart_high_defaulting.png\") \n",
    "\n",
    "pie_charts(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_attribute_subset(table, header, num_values):\n",
    "    '''\n",
    "        Returns a copy table with a random columns removed\n",
    "        Param table: A table to remove attributes from\n",
    "        Param header: The attribute names\n",
    "        Param num_values: The number of attributes to keep\n",
    "        Returns: A tuple with the first item being the table with num_values random attibutes\n",
    "                and the second item a list of the names of the attributes it chose\n",
    "    '''\n",
    "    smaller_table = copy.deepcopy(table)\n",
    "    num_attributes = len(smaller_table[0])\n",
    "    indices_to_remove = random.sample(range(0, num_attributes-1), num_attributes-num_values) \n",
    "    indices_to_remove.sort(reverse=True)\n",
    "    for c in indices_to_remove:\n",
    "        for r, _ in enumerate(smaller_table):\n",
    "            del smaller_table[r][c] \n",
    "        \n",
    "    attributes_kept = [header[i] for i in range(num_attributes) if i not in indices_to_remove]\n",
    "        \n",
    "    return smaller_table, attributes_kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more class 0 than class 1. This zero_R_classifier tests the accuracy (other metrics don't make sense with TP=0) of just predicting 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788\n"
     ]
    }
   ],
   "source": [
    "def zero_r(table):\n",
    "    '''\n",
    "        A zero rules classifier which returns the most common class\n",
    "        Param table:  A table with classifications of instances in the last row\n",
    "        Returns: The most frequent class\n",
    "    '''\n",
    "    classes, counts = utils.get_frequencies(table, -1)\n",
    "    combo = [(counts[i], classes[i]) for i in range(len(classes))]\n",
    "    combo.sort(reverse=True)\n",
    "    return combo[0][1]\n",
    "\n",
    "\n",
    "\n",
    "def zero_R_classifier(table, measurement='a'):\n",
    "    folds = utils.get_stratified_folds(table)\n",
    "    prediction = zero_r(table)\n",
    "\n",
    "    predictions, actuals = [], [] \n",
    "    for i, fold in enumerate(folds):\n",
    "        train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "        test, train = utils.normalize_attributes(fold, train)\n",
    "        for test_instance in test:\n",
    "            predictions.append(prediction)\n",
    "            actuals.append(test_instance[-1])\n",
    "\n",
    "\n",
    "    correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "    return correct.count(True) / len(correct)\n",
    "\n",
    "\n",
    "print(zero_R_classifier(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following cell tests how different k values impact the performace of a kNN classifier. We were surprised by how large K was for optimal results. Before removing ID, these forms of classifier evaluation all tended to be highest in the upper 60 to 100 range, after which they would drop off. After removing ID, around 100 was best for accuracy and recall, but precision and f-measure were much higher, so much so as to be basically using the majority from all training instances. At this point, we created the zero-R classifier above which showed that these Ks were not achieving the same accuracy as simply guessing the majority class, 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing at k=91\n",
      "testing at k=97\n",
      "testing at k=103\n",
      "testing at k=109\n",
      "testing at k=115\n",
      "testing at k=121\n",
      "testing at k=127\n",
      "testing at k=133\n",
      "testing at k=139\n",
      "testing at k=145\n",
      "testing at k=151\n",
      "testing at k=157\n",
      "testing at k=163\n",
      "testing at k=169\n",
      "testing at k=175\n",
      "testing at k=181\n",
      "testing at k=187\n",
      "testing at k=193\n",
      "testing at k=199\n",
      "Accuracies for variable k\n",
      " [(0.736, 91), (0.736, 97), (0.736, 103), (0.736, 109), (0.736, 115), (0.736, 121), (0.736, 127), (0.736, 133), (0.736, 139), (0.704, 145), (0.612, 151), (0.536, 157), (0.516, 163), (0.516, 169), (0.516, 175), (0.516, 181), (0.516, 187), (0.516, 193), (0.516, 199)]\n",
      "sorted [(0.736, 139), (0.736, 133), (0.736, 127), (0.736, 121), (0.736, 115), (0.736, 109), (0.736, 103), (0.736, 97), (0.736, 91), (0.704, 145), (0.612, 151), (0.536, 157), (0.516, 199), (0.516, 193), (0.516, 187), (0.516, 181), (0.516, 175), (0.516, 169), (0.516, 163)]\n"
     ]
    }
   ],
   "source": [
    "def create_kNN_classifier_vary_k(table, start_k=9, end_k=99, step=6, measurement='a'):\n",
    "    '''\n",
    "        This function uses stratified cross fold validation to test different k values for a table.\n",
    "        It can return measurements of accuracy ('a'), recall('r'), precision('p'), or F-measure('f')\n",
    "        Param table: A table to test kNN on\n",
    "        Param start_k: The minimum k value to test.\n",
    "        Param end_k: The maximum k value to test\n",
    "        Param step: The step between k values tested. \n",
    "        Param measurement: The measurement type to return \n",
    "                            accuracy ('a'), recall('r'), precision('p'), or F-measure('f')\n",
    "        Returns: A list of tuples (measurement_value, k)\n",
    "    '''\n",
    "    folds = utils.get_stratified_folds(table)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for k in range(start_k, end_k, step):\n",
    "        print(\"testing at k=%d\" % k)\n",
    "        predictions, actuals = [], [] \n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance[:-1], train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "        \n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            results.append((correct.count(True) / len(correct), k))\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                results.append((true_positives.count(True)/predicted_positives,k))\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                results.append((true_positives.count(True)/actual_positives,k))\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                results.append((2*precision*recall/(precision+recall),k))\n",
    "            else:\n",
    "                print(\"error - invalid measurement\", measurement)\n",
    "                break\n",
    "    return results\n",
    "\n",
    "\n",
    "accuracies_k = create_kNN_classifier_vary_k(table, start_k=49, end_k=100)\n",
    "print(\"Accuracies for variable k\\n\", accuracies_k)\n",
    "accuracies_k.sort(reverse=True)\n",
    "print(\"sorted\", accuracies_k)\n",
    "\n",
    "# recalls_k = create_kNN_classifier_vary_k(table, 91, 110, measurement='r')\n",
    "# # print(\"Recall values for variable k\\n\", recalls_k)\n",
    "# recalls_k.sort(reverse=True)\n",
    "# print(\"sorted\", recalls_k)\n",
    "\n",
    "# precisions_k = create_kNN_classifier_vary_k(table, 91, 110, measurement='p')\n",
    "# # print(\"Precision values for variable k\\n\", precisions_k)\n",
    "# precisions_k.sort(reverse=True)\n",
    "# print(\"sorted\", precisions_k)\n",
    "\n",
    "# f_measures_k = create_kNN_classifier_vary_k(table, 91, 110, measurement='f')\n",
    "# # print(\"F-measure values for variable k\\n\", f_measures_k)\n",
    "# f_measures_k.sort(reverse=True)\n",
    "# print(\"sorted\", f_measures_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classifier uses the best K determined by the previous step and varies which attributes it uses for prediction by taking random attribute subsets of size, default 10, which is passed as a parameter. It does this multiple times and evaluates them using either accuracy, recall, precision, or F-measure. This generally got slightly higher accuracy than just using the best k value, which makes sense since it carries that k value forward; it should improve upon it. However it still wasn't as good as just guessing 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing random attribute set 1 of 30\n",
      "testing random attribute set 2 of 30\n",
      "testing random attribute set 3 of 30\n",
      "testing random attribute set 4 of 30\n",
      "testing random attribute set 5 of 30\n",
      "testing random attribute set 6 of 30\n",
      "testing random attribute set 7 of 30\n",
      "testing random attribute set 8 of 30\n",
      "testing random attribute set 9 of 30\n",
      "testing random attribute set 10 of 30\n",
      "testing random attribute set 11 of 30\n",
      "testing random attribute set 12 of 30\n",
      "testing random attribute set 13 of 30\n",
      "testing random attribute set 14 of 30\n",
      "testing random attribute set 15 of 30\n",
      "testing random attribute set 16 of 30\n",
      "testing random attribute set 17 of 30\n",
      "testing random attribute set 18 of 30\n",
      "testing random attribute set 19 of 30\n",
      "testing random attribute set 20 of 30\n",
      "testing random attribute set 21 of 30\n",
      "testing random attribute set 22 of 30\n",
      "testing random attribute set 23 of 30\n",
      "testing random attribute set 24 of 30\n",
      "testing random attribute set 25 of 30\n",
      "testing random attribute set 26 of 30\n",
      "testing random attribute set 27 of 30\n",
      "testing random attribute set 28 of 30\n",
      "testing random attribute set 29 of 30\n",
      "testing random attribute set 30 of 30\n",
      "\n",
      "Best 5 accuracies for variable attribute subset\n",
      " [(0.776, ['LIMIT_BAL', 'SEX', 'AGE', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'PAY_AMT1', 'PAY_AMT4', 'default payment next month']), (0.772, ['SEX', 'MARRIAGE', 'PAY_4', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT4', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'default payment next month']), (0.772, ['LIMIT_BAL', 'SEX', 'PAY_2', 'PAY_3', 'PAY_5', 'BILL_AMT1', 'BILL_AMT2', 'PAY_AMT1', 'PAY_AMT3', 'default payment next month']), (0.772, ['LIMIT_BAL', 'EDUCATION', 'AGE', 'PAY_5', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT3', 'default payment next month']), (0.768, ['PAY_0', 'PAY_3', 'PAY_6', 'BILL_AMT1', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT5', 'PAY_AMT6', 'default payment next month'])]\n",
      "[0, 1, 4, 8, 9, 10, 11, 17, 20, 23]\n"
     ]
    }
   ],
   "source": [
    "def create_kNN_classifier_vary_attributes(table, header, k, iterations=20, F=10, measurement='a'):\n",
    "    '''\n",
    "        Param table: The table used to test different attributes from\n",
    "        Param k: number of nearest neighbors\n",
    "        Param iterations: number of random subsets of attributes tested\n",
    "        Param F: number of attributes per subset\n",
    "        Param measurement: The measurement type to return \n",
    "                            accuracy ('a'), recall('r'), precision('p'), or F-measure('f')\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    for i in range(iterations):\n",
    "        print(\"testing random attribute set\", i+1, \"of\", iterations)\n",
    "        current_table, current_attribs = get_random_attribute_subset(table, header, F)\n",
    "        folds = utils.get_stratified_folds(current_table)\n",
    "        predictions, actuals = [], []\n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance[:-1], train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            results.append((correct.count(True) / len(correct), current_attribs))\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                results.append((true_positives.count(True)/predicted_positives, current_attribs))\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                results.append((true_positives.count(True)/actual_positives, current_attribs))\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                results.append((2*precision*recall/(precision+recall), current_attribs))\n",
    "    \n",
    "    return results\n",
    "\n",
    "accuracies_a = create_kNN_classifier_vary_attributes(table, header, accuracies_k[0][1], 30)\n",
    "accuracies_a.sort(reverse=True)\n",
    "print(\"\\nBest 5 accuracies for variable attribute subset\\n\", accuracies_a[:5])\n",
    "best_feature_set_indices = [header.index(x) for x in accuracies_a[0][1]]\n",
    "print(best_feature_set_indices)\n",
    "\n",
    "\n",
    "# recalls_a = create_kNN_classifier_vary_attributes(table, header, recalls_k[0][1], 30, measurement='r')\n",
    "# recalls_a.sort(reverse=True)\n",
    "# print(\"\\nBest 5 recall values for variable attribute subset\\n\", recalls_a[:5])\n",
    "# best_feature_set_indices = [header.index(x) for x in recalls_a[0][1]]\n",
    "# print(best_feature_set_indices)\n",
    "\n",
    "# precisions_a = create_kNN_classifier_vary_attributes(table, header, precisions_k[0][1], 30, measurement='r')\n",
    "# precisions_a.sort(reverse=True)\n",
    "# print(\"\\nBest 5 precision values for variable attribute subset\\n\", precisions_a[:5])\n",
    "# best_feature_set_indices = [header.index(x) for x in precisions_a[0][1]]\n",
    "# print(best_feature_set_indices)\n",
    "\n",
    "# f_measures_a = create_kNN_classifier_vary_attributes(table, header, f_measures_k[0][1], 30, measurement='r')\n",
    "# f_measures_a.sort(reverse=True)\n",
    "# print(\"\\nBest 5 precision values for variable attribute subset\\n\", f_measures_a[:5])\n",
    "# best_feature_set_indices = [header.index(x) for x in f_measures_a[0][1]]\n",
    "# print(best_feature_set_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_of_table(table, attributes):\n",
    "    smaller_table = copy.deepcopy(table)\n",
    "    num_attributes = len(smaller_table[0])\n",
    "    indices_to_remove = [i for i in range(num_attributes) if i not in attributes]\n",
    "    indices_to_remove.sort(reverse=True)\n",
    "    for c in indices_to_remove:\n",
    "        for r, _ in enumerate(smaller_table):\n",
    "            del smaller_table[r][c] \n",
    "        \n",
    "    return smaller_table\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_table(table, weights):\n",
    "    '''\n",
    "        Weights the columns of a table by the amount given by weights\n",
    "        Param table: A table of data, recommended normalized\n",
    "        Param weights: The weights to apply by column to the table\n",
    "        Returns: A table with the columns of the original table multipled by the given weights\n",
    "    '''\n",
    "    for r in range(len(table)):\n",
    "        for c in range(len(weights)):\n",
    "            table[r][c] = table[r][c] * weights[c]\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this following cell, using the previously calculated best k and attribute subset, we try different weights for each subset. This method got slightly over zeroR, but not by much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing random attribute weights 1 of 30\n",
      "testing random attribute weights 2 of 30\n",
      "testing random attribute weights 3 of 30\n",
      "testing random attribute weights 4 of 30\n",
      "testing random attribute weights 5 of 30\n",
      "testing random attribute weights 6 of 30\n",
      "testing random attribute weights 7 of 30\n",
      "testing random attribute weights 8 of 30\n",
      "testing random attribute weights 9 of 30\n",
      "testing random attribute weights 10 of 30\n",
      "testing random attribute weights 11 of 30\n",
      "testing random attribute weights 12 of 30\n",
      "testing random attribute weights 13 of 30\n",
      "testing random attribute weights 14 of 30\n",
      "testing random attribute weights 15 of 30\n",
      "testing random attribute weights 16 of 30\n",
      "testing random attribute weights 17 of 30\n",
      "testing random attribute weights 18 of 30\n",
      "testing random attribute weights 19 of 30\n",
      "testing random attribute weights 20 of 30\n",
      "testing random attribute weights 21 of 30\n",
      "testing random attribute weights 22 of 30\n",
      "testing random attribute weights 23 of 30\n",
      "testing random attribute weights 24 of 30\n",
      "testing random attribute weights 25 of 30\n",
      "testing random attribute weights 26 of 30\n",
      "testing random attribute weights 27 of 30\n",
      "testing random attribute weights 28 of 30\n",
      "testing random attribute weights 29 of 30\n",
      "testing random attribute weights 30 of 30\n",
      "\n",
      "Best 5 accuracies for variable weights over attribute subset\n",
      " [(0.784, {'LIMIT_BAL': 0, 'SEX': 5, 'AGE': 5, 'PAY_4': 1, 'PAY_5': 2, 'PAY_6': 5, 'BILL_AMT1': 0, 'PAY_AMT1': 0, 'PAY_AMT4': 0, 'default payment next month': 1}), (0.78, {'LIMIT_BAL': 1, 'SEX': 0, 'AGE': 0, 'PAY_4': 2, 'PAY_5': 5, 'PAY_6': 1, 'BILL_AMT1': 1, 'PAY_AMT1': 2, 'PAY_AMT4': 2, 'default payment next month': 1}), (0.78, {'LIMIT_BAL': 5, 'SEX': 2, 'AGE': 0, 'PAY_4': 0, 'PAY_5': 1, 'PAY_6': 2, 'BILL_AMT1': 1, 'PAY_AMT1': 4, 'PAY_AMT4': 5, 'default payment next month': 1}), (0.78, {'LIMIT_BAL': 3, 'SEX': 1, 'AGE': 0, 'PAY_4': 3, 'PAY_5': 5, 'PAY_6': 3, 'BILL_AMT1': 0, 'PAY_AMT1': 2, 'PAY_AMT4': 1, 'default payment next month': 1}), (0.78, {'LIMIT_BAL': 4, 'SEX': 4, 'AGE': 1, 'PAY_4': 2, 'PAY_5': 1, 'PAY_6': 5, 'BILL_AMT1': 0, 'PAY_AMT1': 4, 'PAY_AMT4': 0, 'default payment next month': 1})]\n"
     ]
    }
   ],
   "source": [
    "def create_kNN_classifier_vary_weights(table, header, attributes, k, iterations=20, measurement='a'):\n",
    "    '''\n",
    "        Param table: The table used for testing classification\n",
    "        Param header: The names of all the attributes\n",
    "        Param attributes: The attribute subset to use\n",
    "        Param k: the number of nearest neighbors to use\n",
    "        Param iterations: The number of random weights to test\n",
    "        Param measurement: The type of measurement to calculate\n",
    "            accuracy ('a'), recall('r'), precision('p'), or F-measure('f')\n",
    "        Returns: A tuple with the calculated measurement value as the first item and a dictionary associating attribute\n",
    "                names with weights for the secon\n",
    "    '''\n",
    "    best_feature_set_indices = [header.index(x) for x in attributes]\n",
    "    table = get_subset_of_table(table, best_feature_set_indices)\n",
    "    results = []\n",
    "    for i in range(iterations):\n",
    "        print(\"testing random attribute weights\", i+1, \"of\", iterations)\n",
    "        folds = utils.get_stratified_folds(table)\n",
    "        predictions, actuals = [], []\n",
    "        weights = [random.randint(0,5) for _ in range(len(best_feature_set_indices)-1)] # weight all but class label (-1)\n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            train = weight_table(train, weights)\n",
    "            test = weight_table(test, weights)\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance[:-1], train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "        weights.append(1)  # weight of class label is just one, so same length as attributes\n",
    "        weights = dict(zip(attributes, weights))  # associate attributes with their weights for results\n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            results.append((correct.count(True) / len(correct), weights))\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                weights = dict(zip(attributes))\n",
    "                results.append((true_positives.count(True)/predicted_positives, weights))\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                results.append((true_positives.count(True)/actual_positives, weights))\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                results.append((2*precision*recall/(precision+recall), weights))\n",
    "        \n",
    "    return results\n",
    "        \n",
    "\n",
    "\n",
    "accuracies_w = create_kNN_classifier_vary_weights(table, header, accuracies_a[0][1], accuracies_k[0][1], 30)\n",
    "accuracies_w.sort(key=lambda x:x[0], reverse=True)\n",
    "print(\"\\nBest 5 accuracies for variable weights over attribute subset\\n\", accuracies_w[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble classifier uses different k-values combined with different random attribute subsets and different random weights to make predictions using a simple majority voting approach. Unfortunately, it ends up being less accurate than the best of its parts. The overall most accurate approach was weighted attribute subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing weak learner 1 of 30\n",
      "Testing weak learner 2 of 30\n",
      "Testing weak learner 3 of 30\n",
      "Testing weak learner 4 of 30\n",
      "Testing weak learner 5 of 30\n",
      "Testing weak learner 6 of 30\n",
      "Testing weak learner 7 of 30\n",
      "Testing weak learner 8 of 30\n",
      "Testing weak learner 9 of 30\n",
      "Testing weak learner 10 of 30\n",
      "Testing weak learner 11 of 30\n",
      "Testing weak learner 12 of 30\n",
      "Testing weak learner 13 of 30\n",
      "Testing weak learner 14 of 30\n",
      "Testing weak learner 15 of 30\n",
      "Testing weak learner 16 of 30\n",
      "Testing weak learner 17 of 30\n",
      "Testing weak learner 18 of 30\n",
      "Testing weak learner 19 of 30\n",
      "Testing weak learner 20 of 30\n",
      "Testing weak learner 21 of 30\n",
      "Testing weak learner 22 of 30\n",
      "Testing weak learner 23 of 30\n",
      "Testing weak learner 24 of 30\n",
      "Testing weak learner 25 of 30\n",
      "Testing weak learner 26 of 30\n",
      "Testing weak learner 27 of 30\n",
      "Testing weak learner 28 of 30\n",
      "Testing weak learner 29 of 30\n",
      "Testing weak learner 30 of 30\n",
      "[[0.784, 99, [0, 7, 16, 19, 22, 23], [0, 0, 0, 3, 3]], [0.784, 45, [0, 1, 6, 9, 14, 17, 23], [3, 2, 0, 2, 0, 3]], [0.784, 129, [10, 14, 19, 20, 23], [1, 1, 5, 5]], [0.776, 65, [0, 4, 8, 12, 23], [3, 1, 2, 2]], [0.776, 57, [6, 11, 16, 21, 23], [0, 2, 5, 1]], [0.772, 113, [1, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23], [2, 4, 3, 3, 5, 3, 2, 4, 4, 0, 3, 3, 2]], [0.772, 111, [1, 5, 7, 8, 10, 11, 12, 15, 17, 21, 23], [1, 2, 0, 5, 0, 5, 2, 5, 3, 3]], [0.768, 69, [5, 6, 8, 10, 19, 20, 23], [1, 5, 3, 4, 3, 5]], [0.764, 43, [1, 7, 9, 11, 18, 23], [0, 4, 2, 1, 3]], [0.764, 51, [1, 7, 8, 9, 10, 12, 13, 16, 17, 19, 21, 22, 23], [1, 4, 5, 2, 0, 0, 1, 2, 2, 0, 2, 2]], [0.76, 81, [3, 4, 7, 10, 19, 20, 23], [3, 1, 1, 4, 2, 5]], [0.76, 65, [0, 6, 13, 22, 23], [5, 1, 2, 3]], [0.76, 41, [2, 6, 8, 12, 16, 23], [1, 5, 3, 3, 5]], [0.76, 97, [1, 3, 5, 6, 12, 13, 15, 16, 18, 20, 21, 23], [5, 0, 4, 4, 5, 4, 1, 4, 3, 0, 0]], [0.756, 129, [1, 2, 5, 6, 7, 9, 12, 13, 14, 16, 18, 20, 22, 23], [4, 0, 0, 1, 4, 2, 4, 3, 3, 1, 4, 3, 0]]]\n",
      "0.764\n"
     ]
    }
   ],
   "source": [
    "def create_ensemble_classifier(table, header, N=30, M=15, measurement='a'):\n",
    "    '''\n",
    "        Creates N weak classifiers and uses majority voting of the best M to make predictions. \n",
    "    '''\n",
    "    weak_classifiers = []  # list of [measurement, k, attribute indices, weights]\n",
    "\n",
    "    for i in range(N):\n",
    "        print(\"Testing weak learner\", i+1, \"of\", N)\n",
    "        k = random.randint(20,70)\n",
    "        k = 2*k + 1 # ensure k is odd\n",
    "        current_table, current_attribs = get_random_attribute_subset(table, header, random.randint(5, 15)) # random attribute subset\n",
    "        best_feature_set_indices = [header.index(x) for x in current_attribs]\n",
    "        weights = [random.randint(0,5) for _ in range(len(best_feature_set_indices)-1)]\n",
    "\n",
    "        folds = utils.get_stratified_folds(current_table)\n",
    "        predictions, actuals = [], []\n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            train = weight_table(train, weights)\n",
    "            test = weight_table(test, weights)\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance[:-1], train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "\n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            weak_classifiers.append([correct.count(True) / len(correct), k, best_feature_set_indices, weights])\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                weights = dict(zip(attributes))\n",
    "                weak_classifiers.append([true_positives.count(True)/predicted_positives, k, best_feature_set_indices, weights])\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                weak_classifiers.append([true_positives.count(True)/actual_positives, k, best_feature_set_indices, weights])\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                weak_classifiers.append([2*precision*recall/(precision+recall), k, best_feature_set_indices, weights])\n",
    "        weak_classifiers.sort(key=lambda x:x[0], reverse=True)\n",
    "\n",
    "    \n",
    "    ensemble = weak_classifiers[:M]\n",
    "    return ensemble\n",
    "    \n",
    "    \n",
    "def test_ensemble_classifier(table, ensemble, measurement='a'):\n",
    "    '''\n",
    "        Param table: A table to test the ensemble classifier over\n",
    "        Param ensemble: A list of weak kNN learners represented by a list [measurement, k, attribute indices, weights]\n",
    "    '''\n",
    "    folds = utils.get_stratified_folds(table)\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    for i, fold in enumerate(folds):\n",
    "            weak_classifiers_predictions = [[] for _ in fold]  # a list of predictions for each test instance\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            for classifier in ensemble:\n",
    "                training_set = get_subset_of_table(train, classifier[2])\n",
    "                test_set = get_subset_of_table(fold, classifier[2])\n",
    "                test_set, training_set = utils.normalize_attributes(test_set, training_set)\n",
    "                training_set = weight_table(training_set, classifier[3])\n",
    "                test_set = weight_table(test_set, classifier[3])\n",
    "                utils.remove_column(training_set, -1) # remove the class column before prediction\n",
    "                utils.remove_column(test_set, -1)\n",
    "                for i, test_instance in enumerate(test_set):\n",
    "                    weak_classifiers_predictions[i].append(utils.make_kNN_prediction(test_instance, training_set, classifier[1]))\n",
    "            predictions.extend([int(np.median(x)) for x in weak_classifiers_predictions])  # binary classification majority voting\n",
    "            actuals.extend(utils.get_column(fold, -1))  # actual class labels\n",
    "    \n",
    "    if measurement == 'a':\n",
    "        correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "        return correct.count(True) / len(correct)\n",
    "    else:\n",
    "        true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "        if measurement == 'r':\n",
    "            predicted_positives = predictions.count(1)\n",
    "            return true_positives.count(True)/predicted_positives\n",
    "        elif measurement == 'p':\n",
    "            actual_positives = actuals.count(1)\n",
    "            return true_positives.count(True)/actual_positives\n",
    "        elif measurement == 'f':\n",
    "            recall = true_positives.count(True) / predictions.count(1)\n",
    "            precision = true_positives.count(True) / actuals.count(1)\n",
    "            return 2*precision*recall/(precision+recall)\n",
    "    \n",
    "random_ensemble = create_ensemble_classifier(table, header)\n",
    "print(random_ensemble)\n",
    "\n",
    "print(test_ensemble_classifier(table, random_ensemble))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88      7060\n",
      "           1       0.00      0.00      0.00      1940\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      9000\n",
      "   macro avg       0.39      0.50      0.44      9000\n",
      "weighted avg       0.62      0.78      0.69      9000\n",
      "\n",
      "[[7060    0]\n",
      " [1940    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88      7060\n",
      "           1       0.33      0.00      0.00      1940\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      9000\n",
      "   macro avg       0.56      0.50      0.44      9000\n",
      "weighted avg       0.69      0.78      0.69      9000\n",
      "\n",
      "[[7058    2]\n",
      " [1939    1]]\n",
      "Train Accuracy ::  0.7763809523809524\n",
      "0.7843333333333333\n",
      "Train Accuracy ::  0.7763333333333333\n",
      "Test Accuracy ::  0.7843333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeff\\Anaconda33\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Jeff\\Anaconda33\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Jeff\\Anaconda33\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def bayes_log():\n",
    "\n",
    "    #X, y = datasets.make_classification(n_samples=100000, n_features=20,n_informative=2, n_redundant=2)\n",
    "    header, table = utils.open_csv_with_header(\"default_of_credit_card_clients.csv\")\n",
    "    df = pd.read_csv('default_of_credit_card_clients.csv')   \n",
    "    training_features = ['SEX', 'EDUCATION','MARRIAGE','AGE']\n",
    "    target = 'default payment next month'\n",
    " \n",
    "    \n",
    "    # Train , Test data split\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df[training_features], df[target], train_size=0.7)\n",
    " \n",
    "    # Training Logistic regression model\n",
    "    trained_logistic_regression_model = train_logistic_regression(train_x, train_y)\n",
    " \n",
    "    train_accuracy = model_accuracy(trained_logistic_regression_model, train_x, train_y)\n",
    " \n",
    "    # Testing the logistic regression model\n",
    "    test_accuracy = model_accuracy(trained_logistic_regression_model, test_x, test_y)\n",
    "    print(metrics.classification_report(test_y,trained_logistic_regression_model.predict(test_x)))\n",
    "    print(metrics.confusion_matrix(test_y,trained_logistic_regression_model.predict(test_x)))\n",
    "    print (\"Train Accuracy :: \", train_accuracy)\n",
    "    print (\"Test Accuracy :: \", test_accuracy)\n",
    "    \n",
    "def train_logistic_regression(train_x, train_y):\n",
    "    \"\"\"\n",
    "    Training logistic regression model with train dataset features(train_x) and target(train_y)\n",
    "    :param train_x:\n",
    "    :param train_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    " \n",
    "    logistic_regression_model = LogisticRegression()\n",
    "    logistic_regression_model.fit(train_x, train_y)\n",
    "    return logistic_regression_model\n",
    "\n",
    "def model_accuracy(trained_model, features, targets):\n",
    "    \"\"\"\n",
    "    Get the accuracy score of the model\n",
    "    :param trained_model:\n",
    "    :param features:\n",
    "    :param targets:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    accuracy_score = trained_model.score(features, targets)\n",
    "    return accuracy_score\n",
    "\n",
    "bayes_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88      7066\n",
      "           1       0.00      0.00      0.00      1934\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      9000\n",
      "   macro avg       0.39      0.50      0.44      9000\n",
      "weighted avg       0.62      0.79      0.69      9000\n",
      "\n",
      "[[7066    0]\n",
      " [1934    0]]\n",
      "0.7851111111111111\n",
      "Train Accuracy ::  0.776\n",
      "Test Accuracy ::  0.7851111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeff\\Anaconda33\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Jeff\\Anaconda33\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "    df = pd.read_csv('default_of_credit_card_clients.csv')   \n",
    "    training_features = ['SEX', 'EDUCATION','MARRIAGE','AGE']\n",
    "    target = 'default payment next month' \n",
    "    # Train , Test data split\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df[training_features], df[target], train_size=0.7)\n",
    "    gnb = GaussianNB() \n",
    "    gnb.fit(train_x,train_y)\n",
    "    y_predict = test_y #target\n",
    "    ypredict = gnb.predict(test_x)\n",
    "    print(metrics.classification_report(y_predict,ypredict))\n",
    "    print(metrics.confusion_matrix(y_predict,ypredict))\n",
    "    print(gnb.score(test_x,test_y))\n",
    "    train_accuracy = model_accuracy(gnb, train_x, train_y)\n",
    " \n",
    "    # Testing the logistic regression model\n",
    "    test_accuracy = model_accuracy(gnb, test_x, test_y)\n",
    "    print (\"Train Accuracy :: \", train_accuracy)\n",
    "    print (\"Test Accuracy :: \", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
