{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alexa Andrews and Jeffrey Mulderink  \n",
    "#### Group name: aa_jm_knn   \n",
    "#### Project title: Improved kNN classifier\n",
    "\n",
    "\n",
    "Our data set was looking to predict whether a person would default on their credit card payment based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "header, table = utils.open_csv_with_header(\"default_of_credit_card_clients.csv\")\n",
    "\n",
    "np.random.shuffle(table)\n",
    "table = table[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_attribute_subset(table, header, num_values):\n",
    "    '''\n",
    "        Returns a copy table with a random columns removed\n",
    "        Param table: A table to remove attributes from\n",
    "        Param header: The attribute names\n",
    "        Param num_values: The number of attributes to keep\n",
    "        Returns: A tuple with the first item being the table with num_values random attibutes\n",
    "                and the second item a list of the names of the attributes it chose\n",
    "    '''\n",
    "    smaller_table = copy.deepcopy(table)\n",
    "    num_attributes = len(smaller_table[0])\n",
    "    indices_to_remove = random.sample(range(0, num_attributes-1), num_attributes-num_values) \n",
    "    indices_to_remove.sort(reverse=True)\n",
    "    for c in indices_to_remove:\n",
    "        for r, _ in enumerate(smaller_table):\n",
    "            del smaller_table[r][c] \n",
    "        \n",
    "    attributes_kept = [header[i] for i in range(num_attributes) if i not in indices_to_remove]\n",
    "        \n",
    "    return smaller_table, attributes_kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following cell tests how different k values impact the performace of a kNN classifier. We were surprised by how large K was for optimal results. These forms of classifier evaluation all tended to be highest in the upper 60 to 100 range, after which they would drop off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing at k=50\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'remove_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-33fdbfbfd9d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0maccuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_kNN_classifier_vary_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;31m# print(\"Accuracies for variable k\\n\", accuracies)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-33fdbfbfd9d8>\u001b[0m in \u001b[0;36mcreate_kNN_classifier_vary_k\u001b[1;34m(table, start_k, end_k, step, measurement)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minstance\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minstance\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# remove the class column before prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtest_instance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_kNN_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_instance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils' has no attribute 'remove_column'"
     ]
    }
   ],
   "source": [
    "def create_kNN_classifier_vary_k(table, start_k=9, end_k=99, step=6, measurement='a'):\n",
    "    '''\n",
    "        This function uses stratified cross fold validation to test different k values for a table.\n",
    "        It can return measurements of accuracy ('a'), recall('r'), precision('p'), or F-measure('f')\n",
    "        Param table: A table to test kNN on\n",
    "        Param start_k: The minimum k value to test.\n",
    "        Param end_k: The maximum k value to test\n",
    "        Param step: The step between k values tested. \n",
    "        Param measurement: The measurement type to return \n",
    "        Returns: A list of tuples (measurement_value, k)\n",
    "    '''\n",
    "    folds = utils.get_stratified_folds(table)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for k in range(start_k, end_k, step):\n",
    "        print(\"testing at k=%d\" % k)\n",
    "        predictions, actuals = [], [] \n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            utils.remove_column(train, -1) # remove the class column before prediction\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance[:-1], train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "        \n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            results.append((correct.count(True) / len(correct), k))\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                results.append((true_positives.count(True)/predicted_positives,k))\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                results.append((true_positives.count(True)/actual_positives,k))\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                results.append((2*precision*recall/(precision+recall),k))\n",
    "            else:\n",
    "                print(\"error - invalid measurement\", measurement)\n",
    "                break\n",
    "    return results\n",
    "\n",
    "accuracies = create_kNN_classifier_vary_k(table, start_k=50, end_k=110)\n",
    "# print(\"Accuracies for variable k\\n\", accuracies)\n",
    "accuracies.sort(reverse=True)\n",
    "print(\"sorted\", accuracies)\n",
    "\n",
    "recalls = create_kNN_classifier_vary_k(table, 50, 110, measurement='r')\n",
    "# print(\"Recall values for variable k\\n\", recalls)\n",
    "recalls.sort(reverse=True)\n",
    "print(\"sorted\", recalls)\n",
    "\n",
    "precisions = create_kNN_classifier_vary_k(table, 50, 110, measurement='p')\n",
    "# print(\"Precision values for variable k\\n\", precisions)\n",
    "precisions.sort(reverse=True)\n",
    "print(\"sorted\", precisions)\n",
    "\n",
    "f_measures = create_kNN_classifier_vary_k(table, 50, 110, measurement='f')\n",
    "# print(\"F-measure values for variable k\\n\", f_measures)\n",
    "f_measures.sort(reverse=True)\n",
    "print(\"sorted\", f_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing random attribute set 1 of 20\n",
      "testing random attribute set 2 of 20\n",
      "testing random attribute set 3 of 20\n",
      "testing random attribute set 4 of 20\n",
      "testing random attribute set 5 of 20\n",
      "testing random attribute set 6 of 20\n",
      "testing random attribute set 7 of 20\n",
      "testing random attribute set 8 of 20\n",
      "testing random attribute set 9 of 20\n",
      "testing random attribute set 10 of 20\n",
      "testing random attribute set 11 of 20\n",
      "testing random attribute set 12 of 20\n",
      "testing random attribute set 13 of 20\n",
      "testing random attribute set 14 of 20\n",
      "testing random attribute set 15 of 20\n",
      "testing random attribute set 16 of 20\n",
      "testing random attribute set 17 of 20\n",
      "testing random attribute set 18 of 20\n",
      "testing random attribute set 19 of 20\n",
      "testing random attribute set 20 of 20\n",
      "\n",
      "Best 5 accuracies for variable attribute subset\n",
      " [(0.9, ['ID', 'SEX', 'PAY_4', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT2', 'PAY_AMT4', 'default payment next month']), (0.884, ['LIMIT_BAL', 'SEX', 'AGE', 'PAY_0', 'PAY_3', 'PAY_4', 'BILL_AMT2', 'PAY_AMT2', 'PAY_AMT5', 'default payment next month']), (0.88, ['LIMIT_BAL', 'SEX', 'MARRIAGE', 'PAY_3', 'PAY_4', 'PAY_5', 'BILL_AMT1', 'BILL_AMT2', 'PAY_AMT4', 'default payment next month']), (0.876, ['ID', 'SEX', 'EDUCATION', 'PAY_3', 'PAY_4', 'BILL_AMT4', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'default payment next month']), (0.868, ['SEX', 'EDUCATION', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT6', 'PAY_AMT2', 'PAY_AMT5', 'default payment next month'])]\n",
      "[0, 2, 9, 13, 14, 16, 17, 19, 21, 24]\n"
     ]
    }
   ],
   "source": [
    "def create_kNN_classifier_vary_attributes(table, header, k, iterations=20, F=10, measurement='a'):\n",
    "    '''\n",
    "        k: nearest neighbors\n",
    "        iterations: number of random subsets of attributes tested\n",
    "        F: number of attributes per subset\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    for i in range(iterations):\n",
    "        print(\"testing random attribute set\", i+1, \"of\", iterations)\n",
    "        current_table, current_attribs = get_random_attribute_subset(table, header, F)\n",
    "        folds = utils.get_stratified_folds(current_table)\n",
    "        predictions, actuals = [], []\n",
    "        for i, fold in enumerate(folds):\n",
    "            train = [instance for fold in folds[:i] for instance in fold] + [instance for fold in folds[i+1:] for instance in fold]\n",
    "            test, train = utils.normalize_attributes(fold, train)\n",
    "            utils.remove_column(train, -1) # remove the class column before prediction\n",
    "            for test_instance in test:\n",
    "                predictions.append(utils.make_kNN_prediction(test_instance[:-1], train, k))\n",
    "                actuals.append(test_instance[-1])\n",
    "        if measurement == 'a':\n",
    "            correct = [predictions[i] == actuals[i] for i in range(len(predictions))]\n",
    "            results.append((correct.count(True) / len(correct), current_attribs))\n",
    "        else:\n",
    "            true_positives = [predictions[i]==1 and actuals[i]==1 for i in range(len(predictions))]\n",
    "            if measurement == 'r':\n",
    "                predicted_positives = predictions.count(1)\n",
    "                results.append((true_positives.count(True)/predicted_positives, current_attribs))\n",
    "            elif measurement == 'p':\n",
    "                actual_positives = actuals.count(1)\n",
    "                results.append((true_positives.count(True)/actual_positives, current_attribs))\n",
    "            elif measurement == 'f':\n",
    "                recall = true_positives.count(True) / predictions.count(1)\n",
    "                precision = true_positives.count(True) / actuals.count(1)\n",
    "                results.append((2*precision*recall/(precision+recall), current_attribs))\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "table = utils.remove_column(table, 0)\n",
    "\n",
    "accuracies = create_kNN_classifier_vary_attributes(table, header, accuracies[0][1], 30)\n",
    "accuracies.sort(reverse=True)\n",
    "print(\"\\nBest 5 accuracies for variable attribute subset\\n\", accuracies[:5])\n",
    "best_feature_set_indices = [header.index(x) for x in accuracies[0][1]]\n",
    "print(best_feature_set_indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kNN_classifier_vary_weights(table, attributes, k): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
